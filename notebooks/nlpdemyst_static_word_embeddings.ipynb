{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlpdemyst-static-word-embeddings.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nitinpunjabi/nlp-demystified/blob/main/notebooks/nlpdemyst_static_word_embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing Demystified | Static Word Embeddings\n",
        "https://nlpdemystified.org<br>\n",
        "https://github.com/nitinpunjabi/nlp-demystified"
      ],
      "metadata": {
        "id": "UuPdnyWXh7vb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPORTANT**<br>\n",
        "Enable **GPU acceleration** by going to *Runtime > Change Runtime Type*.\n",
        "<br><br>\n",
        "Also, if you're running this for free in the cloud rather than using a paid tier or using a local Jupyter server on your machine, then the notebook will *timeout* after a period of inactivity. If that happens and you don't reconnect in time, you will need to upgrade spaCy again and reinstall the requisite statistical package(s).\n",
        "<br><br>\n",
        "Refer to this link on how to run Colab notebooks locally on your machine to avoid this issue:<br>\n",
        "https://research.google.com/colaboratory/local-runtimes.html"
      ],
      "metadata": {
        "id": "IQlvRkb9QyDq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Upgrading spaCy**<br>\n",
        "At the time this notebook was created, spaCy had newer releases but Colab was still using version 2.x by default. So the first step is to upgrade spaCy and download a statistical model for English.\n",
        "\n"
      ],
      "metadata": {
        "id": "BQISrowiKhcS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy==3.*"
      ],
      "metadata": {
        "id": "2vNBSTzIKkRT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NOTE**<br>\n",
        "In this notebook, we won't train standalone word embeddings from scratch. Rather, we'll:\n",
        "1. Use *pretrained* embeddings in one model.\n",
        "2. Train embeddings alongside another model.\n",
        "<br>\n",
        "\n",
        "If you want to try training standalone word embeddings, coding Skip-Gram With Negative Sampling (SGNS) from scratch shouldn't be too hard now that you know all the details. But I recommend just using the **Gensim** library instead:<br>\n",
        "https://radimrehurek.com/gensim/models/word2vec.html<br>\n",
        "https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html\n",
        "\n"
      ],
      "metadata": {
        "id": "SBQlbyiVY3WJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# spaCy Vectors"
      ],
      "metadata": {
        "id": "9XjTQOYRuRbc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unlike previous notebooks where we installed the **en_core_web_sm** (small) model for spaCy, we're going to install the **en_core_web_md** (medium) model instead.\n",
        "<br><br>\n",
        "This is because the small model doesn't come with pretrained word vectors, whereas the medium and large models do.<br>\n",
        "https://spacy.io/models/en#en_core_web_md<br>\n",
        "https://spacy.io/models/en#en_core_web_lg"
      ],
      "metadata": {
        "id": "3mBzXvYP9pN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_md"
      ],
      "metadata": {
        "id": "aAHe_8J981A1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_md')"
      ],
      "metadata": {
        "id": "QspMXugGPQLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the number of word vectors in the package through the **metadata**.<br>\n",
        "https://spacy.io/api/language#meta"
      ],
      "metadata": {
        "id": "ei_pvlI2vBLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.meta['vectors']"
      ],
      "metadata": {
        "id": "YNNDe6Jy_IYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can retrieve a vocabulary entry (a **Lexeme**) and view its raw vector and shape.<br>\n",
        "https://spacy.io/api/lexeme"
      ],
      "metadata": {
        "id": "icRrXecX8EAm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pizza = nlp.vocab['pizza']\n",
        "print(pizza.vector.shape)\n",
        "pizza.vector"
      ],
      "metadata": {
        "id": "AIvfsbG-PeJ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "spaCy **Lexemes** have a *similarity* method to compare vectors.<br>\n",
        "https://spacy.io/api/lexeme#similarity"
      ],
      "metadata": {
        "id": "_P5W2kU39OXe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, we can see related words having a higher similarity measure..."
      ],
      "metadata": {
        "id": "z_aaR4IV9qrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(pizza.similarity(nlp.vocab['tomato']))\n",
        "print(pizza.similarity(nlp.vocab['sauce']))\n",
        "print(pizza.similarity(nlp.vocab['cheese']))"
      ],
      "metadata": {
        "id": "Cy0lQhYs08xV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "...relative to often unrelated words."
      ],
      "metadata": {
        "id": "dtuJ-cu696Yg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(pizza.similarity(nlp.vocab['gorilla']))\n",
        "print(pizza.similarity(nlp.vocab['tree']))\n",
        "print(pizza.similarity(nlp.vocab['yoga']))"
      ],
      "metadata": {
        "id": "Eb_c6ttR080M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Out-of-vocabulary (OOV) words have vectors of zero."
      ],
      "metadata": {
        "id": "m0ghTFWwOVPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.vocab['womblyboo'].vector"
      ],
      "metadata": {
        "id": "WBs6UudgOPwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The vector for any sequence (**doc** or **span**) is simply the average of all the token vectors in the sequence.<br>\n",
        "https://spacy.io/api/doc<br>\n",
        "https://spacy.io/api/span<br>\n",
        "https://spacy.io/usage/linguistic-features#vectors-similarity\n"
      ],
      "metadata": {
        "id": "BqXJ0sfE-XEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d1 = nlp(\"The company has an office in Budapest\")\n",
        "d2 = nlp(\"We have a support team in Hungary\")"
      ],
      "metadata": {
        "id": "VwTsSezl086j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d1.vector"
      ],
      "metadata": {
        "id": "7pZLAtnn089R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The _similarity_ method works for **docs** and **spans** as well."
      ],
      "metadata": {
        "id": "Auikga4B_PAw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(d1[-1:])\n",
        "print(d2[-1:])\n",
        "d1[-1:].similarity(d2[-1:])"
      ],
      "metadata": {
        "id": "yRzuOMk-09AL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(d1[2:5])\n",
        "print(d2[2:5])\n",
        "d1[2:5].similarity(d2[2:5])"
      ],
      "metadata": {
        "id": "eV_vNFJC09DD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "d1.similarity(d2)"
      ],
      "metadata": {
        "id": "0ZLDZeQL09Fr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But comparing similarity based on only token averages can come with issues such as false positives. In this example, the two documents compared clearly have little to nothing in common, but the similarity measurement is relatively high (likely due to stop/generic words)."
      ],
      "metadata": {
        "id": "Srs7eP9-_j9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d3 = nlp(\"dolphins can be pretty mean\")"
      ],
      "metadata": {
        "id": "qQi1Xdf8Frem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(d2)\n",
        "print(d3)\n",
        "d2.similarity(d3)"
      ],
      "metadata": {
        "id": "JJbU0oQKFrXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, creating a sequence vector from token averages throws out word order. So two sentences with identical words which mean different things could score a perfect similarity score."
      ],
      "metadata": {
        "id": "Oiy1tj2xCNrU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d1 = nlp(\"dog bites man\")\n",
        "d2 = nlp(\"man bites dog\")\n",
        "d1.similarity(d2)"
      ],
      "metadata": {
        "id": "2nyFW_6HFrOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doing quick-and-dirty similarity measures like this is probably best if your corpus is domain-specific and similarity is based more on keywords. The more specific, the better.<br><br>\n",
        "For example, a corpus of business news headlines would probably work well."
      ],
      "metadata": {
        "id": "N42bIeZxClOb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d1 = nlp(\"Volkswagen intends to double electric car sales in China\")\n",
        "d2 = nlp(\"First Toyota with solid state battery will be hybrid\")\n",
        "d3 = nlp(\"Dolphins are the thugs of the ocean\")"
      ],
      "metadata": {
        "id": "6l0Eh20SPvHR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(d1.similarity(d2))\n",
        "print(d1.similarity(d3))"
      ],
      "metadata": {
        "id": "G4M_eRqQL4hD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "One way to deal with false positives is to extract key information such as part-of-speech tags and entities, and perform similarity based only on those. In this example, documents 1 and 3 score high on similarity maybe because Chile has a particular dolphin in its waters (that and stop words)."
      ],
      "metadata": {
        "id": "V7PZ__EyDJpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d1 = nlp(\"I want to visit Santiago this winter\")\n",
        "d2 = nlp(\"When is the best time to tour Chile\")\n",
        "d3 = nlp(\"I wouldn't want to run into a dolphin in a dark alley\")"
      ],
      "metadata": {
        "id": "OG4LcoJRxaQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(d1.similarity(d2))\n",
        "print(d1.similarity(d3))"
      ],
      "metadata": {
        "id": "M-5KraiWxbo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have a function which removes stop words and retains only verbs, nouns, and entities."
      ],
      "metadata": {
        "id": "RPqZQX-jESv4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_text(s):\n",
        "  d = nlp(s)\n",
        "  tokens = [t.text for t in d if \n",
        "            not t.is_stop \n",
        "            and (t.tag_ == 'VB' or t.pos_ == 'NOUN' or t.ent_type_ != '')]\n",
        "  return nlp(\" \".join(tokens))"
      ],
      "metadata": {
        "id": "0QQAi6sCEZsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we filter the same three previous sentences again, we see the similarity scores update to something more sensible. The similarity between document 1 and the rest of the documents is lowered, but the score against document 3 is now relatively lower compared to 2."
      ],
      "metadata": {
        "id": "WZerPtzFFgI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d1 = filter_text(\"I want to visit Santiago this winter\")\n",
        "d2 = filter_text(\"When is the best time to tour Chile\")\n",
        "d3 = filter_text(\"I wouldn't want to run into a dolphin in a dark alley\")"
      ],
      "metadata": {
        "id": "Up1efxzHFfEp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(d1.similarity(d2))\n",
        "print(d1.similarity(d3))"
      ],
      "metadata": {
        "id": "gaTcV5nvFoDp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also, simple similarity doesn't capture things like *intent*. In this example, someone interested in visiting Santiago or the best time to tour Chile might be interested in cheap flight tickets to the region as well.<br><br>\n",
        "This isn't a fair comparison because the method isn't built for that, but it's just something to keep in mind. The resulting similarity scores actually aren't too bad, but having additional metadata like where the person currently is or doing specific preprocessing would be useful."
      ],
      "metadata": {
        "id": "EwJtGxBaGN8Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "d4 = filter_text(\"Discount flight tickets to South America\")"
      ],
      "metadata": {
        "id": "G6zz_PwxGAih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(d1.similarity(d4))\n",
        "print(d2.similarity(d4))"
      ],
      "metadata": {
        "id": "CY0QsDBXGCZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can modify spaCy's vectors, add your own, and also load third-party vectors rather than using spaCy's built-in vectors:<br>\n",
        "https://spacy.io/api/vectors"
      ],
      "metadata": {
        "id": "NVeP5-nzKlSZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Third-Party Vectors"
      ],
      "metadata": {
        "id": "A_rgKP1vJ2kT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are a variety of pretrained, static word vector packages out there. In this section, we'll use the **Google News** vectors, a collection of three million, 300-dimension word vectors trained from three billion words from a Google News corpus (circa 2015).<br><br>\n",
        "We could technically load these vectors into spaCy, but we'll use **gensim** here instead because it's easier and the library offers an API to do a few cool things with word vectors.\n"
      ],
      "metadata": {
        "id": "RHMDs5FPKXUx"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1t-eUJvZCGiw"
      },
      "source": [
        "# Upgrade gensim just in case.\n",
        "!pip install -U gensim==4.*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll need to first download the actual word vectors. It's over a gigabyte but will fit well within the space constraints of our environment."
      ],
      "metadata": {
        "id": "lQEAXBoBMidI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P /root/input/ -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\""
      ],
      "metadata": {
        "id": "dIKSA-KMbj8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_file = '/root/input/GoogleNews-vectors-negative300.bin.gz'"
      ],
      "metadata": {
        "id": "6FtVSlPQbj4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we'll have **gensim** load the vectors through the **KeyedVectors** module which will enable us to look up vectors by tokens and indices.<br>\n",
        "https://radimrehurek.com/gensim/models/keyedvectors.html\n",
        "<br><br>\n",
        "To save time and space, we'll limit ourselves to 200,000 word vectors for now."
      ],
      "metadata": {
        "id": "BXQRngPaPWhE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.keyedvectors import KeyedVectors"
      ],
      "metadata": {
        "id": "XuvxqVjpMvKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "word_vectors = KeyedVectors.load_word2vec_format(embedding_file, binary=True, limit=200000)"
      ],
      "metadata": {
        "id": "dmRUiKPObjqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Retrieving a word's vector is a matter of using a token as a key."
      ],
      "metadata": {
        "id": "ir22T0t3QqPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors['cell']"
      ],
      "metadata": {
        "id": "2uUdbz0qz60A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The *most_similar* method returns the words with the closest vectors.<br>\n",
        "https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.most_similar"
      ],
      "metadata": {
        "id": "aWsFWJUXQ9RR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors.most_similar(positive=['cell'], topn=10)"
      ],
      "metadata": {
        "id": "2f2CXSQNxZgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also add vectors first, and retrieve the words most similar to that summation. Here, we're adding the vectors for 'cell' and 'phone' and retrieving the vectors closest to that result. "
      ],
      "metadata": {
        "id": "1YN45ETCReXK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors.most_similar(positive=['cell', 'phone'], topn=10)"
      ],
      "metadata": {
        "id": "wvOkR7UWx1su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given a collection of words, the *doesn't_match* method returns the word that doesn't go with the rest (i.e. with the vector that's furthest away from the mean of all the other vectors).<br>\n",
        "https://radimrehurek.com/gensim/models/keyedvectors.html#gensim.models.keyedvectors.KeyedVectors.doesnt_match"
      ],
      "metadata": {
        "id": "PjMdhqJvR7ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors.doesnt_match([\"apple\", \"orange\", \"hamburger\", \"banana\", \"kiwi\"])"
      ],
      "metadata": {
        "id": "oA2NVKHPzExZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see the power of context in this example with 'Toyota' being correctly identified as the odd one out."
      ],
      "metadata": {
        "id": "Xajz9UFQR91b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors.doesnt_match([\"Microsoft\", \"Apple\", \"Toyota\", \"Amazon\", \"Netflix\", \"Google\"])"
      ],
      "metadata": {
        "id": "evIAtmcVygyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing word vectors is straight-forward and can offer insights into what kind of contexts the training algorithm picked up.<br><br>\n",
        "Because these word vectors have a dimension of 300, we need to reduce them down to two dimensions to plot them on a regular graph. This can be done through **Principal Components Analysis (PCA)**:<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html<br>\n",
        "<br>\n",
        "Here, we're plotting the words we considered in the slides."
      ],
      "metadata": {
        "id": "h6UyREbVUS6p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "0ogjtcDVEd3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_pca_scatterplot(model, words):        \n",
        "    word_vectors = np.array([model[w] for w in words])\n",
        "\n",
        "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
        "    \n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r', s=128)\n",
        "    for word, (x,y) in zip(words, twodim):\n",
        "        plt.text(x+0.05, y+0.05, word)"
      ],
      "metadata": {
        "id": "wC1e7EF0-b-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_pca_scatterplot(word_vectors, ['swim', 'swimming', 'cat', 'dog', 'feline', 'road', 'car', 'bus'])"
      ],
      "metadata": {
        "id": "cqmNesyaEhq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can even solve analogies (to a limited extent) with vector arithmetic.<br><br>\n",
        "Here, we're solving the analogy:<br>\n",
        "_Rome is to Italy as London is to __________.<br><br>\n",
        "Arithmetically, this is Italy + London - Rome.\n"
      ],
      "metadata": {
        "id": "EXE5AEEUSnEN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_vectors.most_similar(positive=['Italy', 'London'], negative=['Rome'], topn=3)"
      ],
      "metadata": {
        "id": "VCJZYi-1zNUI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing it can help with geometric intuition."
      ],
      "metadata": {
        "id": "vunBH2R5XRWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "display_pca_scatterplot(word_vectors, ['Rome', 'Italy', 'London', 'Britain'])"
      ],
      "metadata": {
        "id": "RlH9W8q2V0Dm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using Pretrained Word Vectors for Classification"
      ],
      "metadata": {
        "id": "iuDWv6NXXjoF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we'll train a **Keras** model to use these Google News vectors to perform sentiment analysis on a bunch of **Yelp** reviews.\n",
        "<br><br>\n",
        "For this model, we'll increase the number of word vectors loaded to 1,000,000.\n",
        "\n"
      ],
      "metadata": {
        "id": "Am-9bvppYKuS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "word_vectors = KeyedVectors.load_word2vec_format(embedding_file, binary=True, limit=1000000)"
      ],
      "metadata": {
        "id": "VVLj4fSMzvHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset we'll use is *Yelp Polarity Reviews*, a collection of ~600,000 reviews for both training and testing.<br><br>\n",
        "The original Yelp reviews use a five-star rating system. The ratings in this dataset have been modified to simply be negative (label==1) or positive (label==2).<br>\n",
        "https://www.tensorflow.org/datasets/catalog/yelp_polarity_reviews<br><br>\n",
        "Tensorflow comes with a datasets loader but we're going to download the file manually and process the data ourselves for completeness."
      ],
      "metadata": {
        "id": "vdZQNavVYxfP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -P /root/input/ -c \"https://s3.amazonaws.com/fast-ai-nlp/yelp_review_polarity_csv.tgz\""
      ],
      "metadata": {
        "id": "RcMEE11bzz_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Unzipping the archive results in *train.csv* and *test.csv* files placed in the default *contents* folder of our environment."
      ],
      "metadata": {
        "id": "wnTnApF4aq7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!tar xvzf /root/input/yelp_review_polarity_csv.tgz\n",
        "\n",
        "# Show current working directory.\n",
        "!pwd"
      ],
      "metadata": {
        "id": "-UK1y9kjzz7y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **Pandas** library makes it simple to load a CSV file into memory and manipulate the data.<br>\n",
        "https://pandas.pydata.org/<br>\n",
        "https://pandas.pydata.org/docs/<br>\n",
        "https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html?highlight=read_csv"
      ],
      "metadata": {
        "id": "OaJCSM3lbK-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "3BNZL6c7zz4B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we're loading the CSV into a Pandas **dataframe** (sort of like an in-memory table) and giving the columns names.<br>\n",
        "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html?highlight=dataframe#pandas.DataFrame"
      ],
      "metadata": {
        "id": "xIG4UVMkbq9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yelp_train = pd.read_csv('yelp_review_polarity_csv/train.csv', names=['sentiment', 'review'])\n",
        "yelp_train.shape"
      ],
      "metadata": {
        "id": "tdLoK1Jozz0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can get a quick view of the data through the *head* method.<br>\n",
        "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.head.html"
      ],
      "metadata": {
        "id": "lGgJl1QCb738"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yelp_train.head()"
      ],
      "metadata": {
        "id": "jVHtVqOBzzwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To save on training time, we'll train on 100,000 reviews rather than the full set. To do that, we'll shuffle the dataset using the *sample* method and *copy* the first 100,000 entries. The reason to shuffle first is to ensure we get a mix of reviews from a variety of businesses (in case the data is sorted in some way).<br>\n",
        "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sample.html<br>\n",
        "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.copy.html\n"
      ],
      "metadata": {
        "id": "9wS0o4gEclCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_SIZE = 100000\n",
        "yelp_train = yelp_train.sample(frac=1, random_state=1)[:TRAIN_SIZE].copy()\n",
        "yelp_train.shape"
      ],
      "metadata": {
        "id": "YacSH4F0c5aU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next thing to do is adjust the labels. This is a **binary classification problem**, so our model's output layer will be a single unit with a **sigmoid** activation function. This function's output will be between 0 and 1 which is then compared against the training label. But the labels are currently 1 for negative, and 2 for positive, which is going to cause problems when calculating the loss.<br><br>\n",
        "So we'll simply replace the 1s with 0s, and 2s with 1s using the *replace* method.<br>\n",
        "https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.replace.html\n",
        "<br><br>\n",
        "Alternatively, we could keep the labels as-is and treat this as a **multiclassification** problem with two labels and use a **softmax**, but we would then need to **one-hot encode** the labels.\n"
      ],
      "metadata": {
        "id": "DLraxnTMd9iE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yelp_train['sentiment'].replace(to_replace=1, value=0, inplace=True)\n",
        "yelp_train['sentiment'].replace(to_replace=2, value=1, inplace=True)"
      ],
      "metadata": {
        "id": "kbmYKm-SjGnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yelp_train.head()"
      ],
      "metadata": {
        "id": "LlzWLECijrg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we've done throughout this course, we'll create train/validation splits.<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
      ],
      "metadata": {
        "id": "rILnYdQMgIlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "yelp_train_split, yelp_val_split = train_test_split(yelp_train, train_size=0.85, random_state=1)"
      ],
      "metadata": {
        "id": "CC-VSNcn7Cpq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up training data.\n",
        "train_reviews = yelp_train_split['review']\n",
        "y_train = np.array(yelp_train_split['sentiment'])\n",
        "\n",
        "# Set up validation data.\n",
        "val_reviews = yelp_val_split['review']\n",
        "y_val = np.array(yelp_val_split['sentiment'])"
      ],
      "metadata": {
        "id": "9Xj4WyLb7Wdr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A quick sanity check to see how our data is distributed (e.g. balanced or skewed)."
      ],
      "metadata": {
        "id": "CK9V0dJAgbSJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "collections.Counter(y_train)"
      ],
      "metadata": {
        "id": "MHBWB2vc7ie1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Because we're relying more on richer encodings (in this case, word vectors), we won't perform as much preprocessing this time around. We'll stick with using the regular Keras **tokenizer** and just filter out numbers and certain symbols.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer<br><br>\n",
        "We'll also have the tokenizer limit itself to tokenizing only the most frequent 20,000 words. This way, the model will focus on the most frequent descriptive sentiment words."
      ],
      "metadata": {
        "id": "M0fM5vRygqfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "tokenizer = keras.preprocessing.text.Tokenizer(num_words=20000, filters='0123456789!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)"
      ],
      "metadata": {
        "id": "UI4FgHPa4p2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build the vocabulary."
      ],
      "metadata": {
        "id": "gXQXX82chivF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "tokenizer.fit_on_texts(train_reviews)"
      ],
      "metadata": {
        "id": "ayGLszCqzzoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to vectorize our reviews. In the [_Neural Network Foundations_](https://github.com/nitinpunjabi/nlp-demystified/blob/main/notebooks/nlpdemyst_neural_network_foundations.ipynb) notebook, we used the *texts_to_matrix* method to turn text into binary bags of words.<br><br>\n",
        "Here, we're going to use the *text_to_sequences* method to turn each review into a sequence of integers, with each integer representing its corresponding token.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#texts_to_sequences\n",
        "\n"
      ],
      "metadata": {
        "id": "v5zDD6RCBzxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "x_train = tokenizer.texts_to_sequences(train_reviews)"
      ],
      "metadata": {
        "id": "OAd431jp6Ydg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The first review in the training set, vectorized.\n",
        "print(x_train[0])"
      ],
      "metadata": {
        "id": "PPo0mNRB6YZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can look up the corresponding tokens using the tokenizer's *index_word* dict. Here are the tokens corresponding to the first three integers from the first vectorized review."
      ],
      "metadata": {
        "id": "kH4I4SeoqVDn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "[tokenizer.index_word[x] for x in x_train[0][:3]]"
      ],
      "metadata": {
        "id": "jkOI7jYFqb44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also convert the integer sequence back to text using the *sequences_to_texts* method, and compare it against the original text.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#sequences_to_texts"
      ],
      "metadata": {
        "id": "ahgfyZhDrHDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Review reconstructed from integer sequence.\n",
        "tokenizer.sequences_to_texts([x_train[0]])"
      ],
      "metadata": {
        "id": "RPlvLdz8rGZZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Original review text.\n",
        "train_reviews.iloc[0]"
      ],
      "metadata": {
        "id": "ZirHR4_OC87H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Some models and situations require us to **pad** our sequences to the same length. While that's not the case here, it can still be beneficial to have all our inputs (and consequently, our batches) to be of uniform size to help with optimizations.<br><br>\n",
        "In this case, we'll make all our reviews 200 tokens in length (in practice, you can choose a number based on some analysis). So the reviews longer than 200 tokens will be truncated, while the reviews shorter than 200 will be padded with zeroes.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences"
      ],
      "metadata": {
        "id": "bEqDNjactNk1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_REVIEW_LEN = 200\n",
        "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=MAX_REVIEW_LEN)"
      ],
      "metadata": {
        "id": "2lTEjMgDqIPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train[0])\n",
        "print(x_train[1])"
      ],
      "metadata": {
        "id": "d2tHPfAVv37k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our training set is prepared. We can now also vectorize and pad our validation set."
      ],
      "metadata": {
        "id": "nBAg3laKwDWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_val = tokenizer.texts_to_sequences(val_reviews)\n",
        "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=MAX_REVIEW_LEN)"
      ],
      "metadata": {
        "id": "E9P72JJYHqKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to incorporate the Google News vectors (currently loaded into gensim) into our Keras model. What we'll do is create an embedding matrix that maps each tokenizer integer to its respective word vector.<br><br>\n",
        "For example, here's the index for the word \"good\" from the Keras tokenizer and the word vector for \"good\" from gensim. We want a matrix which maps the index to the vector.\n"
      ],
      "metadata": {
        "id": "P8dy9OWMwafM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.word_index['good'])"
      ],
      "metadata": {
        "id": "5m7OW6-pwaJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Part of the vector for the word 'good'.\n",
        "print(word_vectors['good'][:50])"
      ],
      "metadata": {
        "id": "3rOAy_IkHqfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll create this embedding matrix by first initializing a matrix of zeros, then looping over every word in the tokenizer vocabulary and:\n",
        "1. Checking if the word has a corresponding vector in gensim.\n",
        "2. If it does, then copy the vector into the matrix row corresponding to the word's index."
      ],
      "metadata": {
        "id": "elzisAtxyO-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# + 1 to account for padding token.\n",
        "num_tokens = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Initialize a matrix of zeroes of size: vocabulary x embedding dimension.\n",
        "embedding_dim = 300\n",
        "embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "\n",
        "for word, i in tokenizer.word_index.items():\n",
        "  if word_vectors.has_index_for(word):\n",
        "    embedding_matrix[i] = word_vectors[word].copy()\n"
      ],
      "metadata": {
        "id": "Iw25Irs4J-JG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick visual check.\n",
        "print(embedding_matrix[tokenizer.word_index['good']][:50])"
      ],
      "metadata": {
        "id": "uncvu4lcJ-OI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're ready to build our first model using pretrained word vectors. The first layer we'll add is a Keras **embedding** layer which is essentially a trainable lookup table/matrix.<br>\n",
        "https://keras.io/api/layers/base_layer/#layer-class<br>\n",
        "https://keras.io/api/layers/core_layers/embedding/<br><br>\n",
        "In this case, we'll populate the **embedding** layer with the embedding matrix we created, and set *trainable* to True. This means we'll allow the model to adjust/fine-tune the word vectors as needed for greater accuracy. This corresponds to one of the scenarios we covered in the slides."
      ],
      "metadata": {
        "id": "CmzNcyFu0Noo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import layers"
      ],
      "metadata": {
        "id": "p-mdJuUeJ-W5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = layers.Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    input_length=MAX_REVIEW_LEN,\n",
        "    trainable=True\n",
        ")"
      ],
      "metadata": {
        "id": "ALs6rkTnJ-aj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use a simple architecture for this model. Here are a few things to note:<br>\n",
        "1. Each training example is a sequence of *integers* which gets converted to a sequence of *vectors*., but subsequent layers are expecting one vector per example. So we're inserting a **GlobalAveragePooling1D** layer after the embedding layer to average out all the word vectors into a single vector, before sending it further into the network. For classification, this can be pretty effective as a base model approach.\n",
        "2. There was no science behind choosing 128 units in the first hidden layer and 64 units in the second hidden layer. The intuition was that signal would be distilled from 300 dimensions down to 128 dimensions, then down to 64 dimensions before going to output.<br>\n",
        "\n",
        "https://keras.io/api/layers/pooling_layers/global_average_pooling1d/\n",
        "<br><br>\n",
        "When we call the model's *summary* method, note how there are no params for the **GlobalAveragePooling1D** layer."
      ],
      "metadata": {
        "id": "OqRgEWpb2iuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(layers.GlobalAveragePooling1D())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "1RrLd4gzbhVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We won't use **early stopping** for this run. This way, we'll be able to compare metrics between the train and validation sets."
      ],
      "metadata": {
        "id": "AwGYMD028zhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "id": "Lz1GmguMcEic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_train_vs_val_performance(history):\n",
        "  training_losses = history.history['loss']\n",
        "  validation_losses = history.history['val_loss']\n",
        "\n",
        "  training_accuracy = history.history['accuracy']\n",
        "  validation_accuracy = history.history['val_accuracy']\n",
        "\n",
        "  epochs = range(1, len(training_losses) + 1)\n",
        "\n",
        "  import matplotlib.pyplot as plt\n",
        "  fig, (ax1, ax2) = plt.subplots(2)\n",
        "  fig.set_figheight(15)\n",
        "  fig.set_figwidth(15)\n",
        "  fig.tight_layout(pad=5.0)\n",
        "\n",
        "  # Plot training vs. validation loss.\n",
        "  ax1.plot(epochs, training_losses, 'bo', label='Training Loss')\n",
        "  ax1.plot(epochs, validation_losses, 'b', label='Validation Loss')\n",
        "  ax1.title.set_text('Training vs. Validation Loss')\n",
        "  ax1.set_xlabel('Epoch')\n",
        "  ax1.set_ylabel('Loss')\n",
        "  ax1.legend()\n",
        "\n",
        "  # PLot training vs. validation accuracy.\n",
        "  ax2.plot(epochs, training_accuracy, 'bo', label='Training Accuracy')\n",
        "  ax2.plot(epochs, validation_accuracy, 'b', label='Validation Accuracy')\n",
        "  ax2.title.set_text('Training vs. Validation Accuracy')\n",
        "  ax2.set_xlabel('Epoch')\n",
        "  ax2.set_ylabel('Accuracy')\n",
        "  ax2.legend()\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "Neard5tEGwnG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_train_vs_val_performance(history)"
      ],
      "metadata": {
        "id": "lbSk2qE1cEfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll initialize a new embedding layer and model and train for epochs equalling the point where we saw the validation loss diverge from the training loss."
      ],
      "metadata": {
        "id": "xdeEdLD6W91U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_layer = layers.Embedding(\n",
        "    num_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    input_length=MAX_REVIEW_LEN,\n",
        "    trainable=True\n",
        ")\n",
        "\n",
        "model = keras.Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(layers.GlobalAveragePooling1D())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train, epochs=<DESIRED_EPOCHS>, batch_size=512)"
      ],
      "metadata": {
        "id": "V_nWiYy6cEbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a trained model, let's try it on the test data. As we did with the training data, we'll:\n",
        "1. Replace the labels with 0 for negative sentiment, and 1 for positive.\n",
        "2. Convert the reviews into a sequence of integers and pad/truncate each review to a fixed length."
      ],
      "metadata": {
        "id": "_4X_sdScYPAP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "yelp_test = pd.read_csv('yelp_review_polarity_csv/test.csv', names=['sentiment', 'review'])"
      ],
      "metadata": {
        "id": "AUsripS7SZdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yelp_test['sentiment'].replace(to_replace=1, value=0, inplace=True)\n",
        "yelp_test['sentiment'].replace(to_replace=2, value=1, inplace=True)\n",
        "yelp_test"
      ],
      "metadata": {
        "id": "bm4Day_jYbTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = np.array(yelp_test['sentiment'])\n",
        "y_test"
      ],
      "metadata": {
        "id": "Qks0uXyzYbTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = tokenizer.texts_to_sequences(yelp_test['review'])"
      ],
      "metadata": {
        "id": "-UyBts6EYbTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = keras.preprocessing.sequence.pad_sequences(x_test, maxlen=MAX_REVIEW_LEN)"
      ],
      "metadata": {
        "id": "loh2rh3WYbTe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "yKmqmLUkYbTf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not bad for a conceptually simple model where we average out a review's word vectors, run it through a few plain hidden layers, and out through a sigmoid function with no regularization and just using defaults for model components (e.g. optimizer settings).<br><br>\n",
        "We can now use the model for predictions."
      ],
      "metadata": {
        "id": "9GBMe-YKZCDP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentiment(reviews):\n",
        "  seqs = tokenizer.texts_to_sequences(reviews)\n",
        "  seqs = keras.preprocessing.sequence.pad_sequences(seqs, maxlen=MAX_REVIEW_LEN)\n",
        "  return model.predict(seqs)\n"
      ],
      "metadata": {
        "id": "rNbNyr8PVJMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Real reviews from Google Reviews.\n",
        "pos_review = \"The best seafood joint in East Village San Diego!  Great lobster roll, great fish, great oysters, great bread, great cocktails, and such amazing service.  The atmosphere is top notch and the location is so much fun being located just a block away from Petco Park (San Diego Padres Stadium).\"\n",
        "neg_review = \"A thoroughly disappointing experience. When you book a Marriott you expect a certain standard. Albany falls way short. Room cleaning has to be booked 24 hours in advance but nobody thought to mention this at check in. The hotel is tired and needs a face-lift. The only bright light in a sea of mediocrity were the pancakes at breakfast. Sadly they weren't enough to save the experience. If you travel to Albany, then do yourself a big favour and book the Westin.\""
      ],
      "metadata": {
        "id": "Jy-4MWV1VJRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment([pos_review, neg_review])"
      ],
      "metadata": {
        "id": "wH8b-Z3GVJU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training New Embeddings and a Model at the Same Time"
      ],
      "metadata": {
        "id": "EUX0AZqYbt1y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this last model, rather than using pretrained embeddings, we'll start with a **random** embedding matrix and let the model come up with its own vectors simultaneously while fitting the training data.<br><br>\n",
        "We'll also use **early stopping**, but otherwise keep everything else the same."
      ],
      "metadata": {
        "id": "-ek2bnbCVJbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential()\n",
        "\n",
        "# The 'trainable' property is True by default.\n",
        "model.add(layers.Embedding(input_dim=num_tokens, \n",
        "                           output_dim=embedding_dim, \n",
        "                           input_length=MAX_REVIEW_LEN))\n",
        "\n",
        "\n",
        "model.add(layers.GlobalAveragePooling1D())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "es_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
        "history = model.fit(x_train, y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val), callbacks=[es_callback])"
      ],
      "metadata": {
        "id": "wCy90PyxVJfK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "5dL0GwLUVJio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like in this case, we get comparable performance between fine-tuning pretrained vectors and training embeddings from scratch as part of the model; likely because of the nature of the data and amount of it."
      ],
      "metadata": {
        "id": "n3Uy8zP-0WUI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try This"
      ],
      "metadata": {
        "id": "Si2Nx7NSfl1V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our first model, we used pretrained vectors in the **embedding layer** and set the *trainable* property to **True**, allowing the model to fine-tune the word vectors.<br><br>\n",
        "Instantiate the same model but this time, set the *trainable* property in the **embedding layer** to **False**. What happens to training performance? Does the training speed increase or decrease? What happens if you try to add some regularization like dropout?"
      ],
      "metadata": {
        "id": "FsQYls6a0zH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the embedding layer.\n",
        "\n",
        "\n",
        "model = keras.Sequential()\n",
        "\n",
        "# Add layers.\n",
        "\n",
        "\n",
        "# Compile model.\n",
        "\n",
        "\n",
        "# Call fit.\n",
        "\n",
        "\n",
        "# Evaluate the model.\n"
      ],
      "metadata": {
        "id": "28u6ssTzflcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Alternative Static Embedding Algorithms"
      ],
      "metadata": {
        "id": "TgJM0Flw6Imq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GloVe\n",
        "**GloVe (Global Vectors for Word Representation)** is another algorithm for creating static word vectors. You can read the original GloVe paper and download pretrained word vectors here:<br>\n",
        "https://nlp.stanford.edu/projects/glove/"
      ],
      "metadata": {
        "id": "Xcq27-hYKdMh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Doc2Vec\n",
        "An algorithm which represents a document as a dense vector which addresses weaknesses of bag-of-words models.<br>\n",
        "https://arxiv.org/abs/1405.4053<br>\n",
        "https://radimrehurek.com/gensim/models/doc2vec.html<br>"
      ],
      "metadata": {
        "id": "331buv_FK4nQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## fastText\n",
        "An alternative approach to creating embeddings. Instead of assigning a vector to each _word_ (e.g. a separate vector each for \"dog\" and \"dogs\"), a vector is assigned to each _subword_. For fastText, a subword is defined as a character n-gram.\n",
        "<br><br>\n",
        "So if n=3, then a word like \"hello\" would result in vectors for \"<he\", \"hel\", \"ell\", \"llo\", \"lo>\" (note that \"<\" and \">\" are special characters). The vector for \"hello\" would be the sum of all the above vectors. This helps deal with OOV situations because vectors can still be assigned to unseen words as long as the n-grams exist in the vocabulary.<br>\n",
        "https://fasttext.cc/<br>\n",
        "https://radimrehurek.com/gensim/models/fasttext.html\n",
        "<br><br>\n",
        "**We'll cover subword tokenization in greater detail later in the course.**"
      ],
      "metadata": {
        "id": "aRq8HklIg2Gu"
      }
    }
  ]
}