{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlpdemyst-neural-network-foundations.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nitinpunjabi/nlp-demystified/blob/main/notebooks/nlpdemyst_neural_network_foundations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Natural Language Processing Demystified | Neural Networks Foundations\n",
        "https://nlpdemystified.org<br>\n",
        "https://github.com/nitinpunjabi/nlp-demystified"
      ],
      "metadata": {
        "id": "UuPdnyWXh7vb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "At the time this notebook was created, spaCy had newer releases but Colab was still using version 2.x by default. So the first step is to upgrade spaCy and download a statistical model for English.\n",
        "<br><br>\n",
        "**IMPORTANT**<br>\n",
        "If you're running this for free in the cloud rather than using a paid tier or using a local Jupyter server on your machine, then the notebook will *timeout* after a period of inactivity. If that happens and you don't reconnect in time, you will need to upgrade spaCy again and reinstall the requisite statistical package(s).\n",
        "<br><br>\n",
        "Refer to this link on how to run Colab notebooks locally on your machine to avoid this issue:<br>\n",
        "https://research.google.com/colaboratory/local-runtimes.html"
      ],
      "metadata": {
        "id": "tjryBafNh_kK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U spacy==3.*\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "gu503I5hiB_h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this demo, we're once again going to use the **20 newgroups** dataset. This is so we can see how a neural network approach compares against our previous model.<br>\n",
        "https://scikit-learn.org/stable/datasets/real_world.html#the-20-newsgroups-text-dataset"
      ],
      "metadata": {
        "id": "8g-gYfUZsHld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the *train* dataset without headers, footers, and quotes to make the problem more challenging.\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "train_corpus = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'))"
      ],
      "metadata": {
        "id": "eJuHSA7PsaSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "pZGdVEev21To"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to use **Tensorflow/Keras** to build our model, but stick with spaCy for text preprocessing. While Keras does come with a basic tokenizer, it lacks spaCy's useful, specialist linguistic features.\n",
        "<br><br>\n",
        "To that end, we'll load the small English statistical model and create a tokenizer function as we did in the previous videos."
      ],
      "metadata": {
        "id": "_ZUR7C6ck5Rd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "f21k1X4SiD9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We don't need named entity recognition nor parsing. Removing them will speed up processing.\n",
        "unwanted_pipes = ['ner', 'parser']\n",
        "\n",
        "def spacy_tokenizer(doc):\n",
        "  with nlp.disable_pipes(*unwanted_pipes):\n",
        "    return [t.lemma_.lower() for t in nlp(doc) if \\\n",
        "            len(t) > 2 and \\\n",
        "            not t.is_punct and \\\n",
        "            not t.is_space and \\\n",
        "            not t.is_stop and \\\n",
        "            t.is_alpha]"
      ],
      "metadata": {
        "id": "0Gb0ocUvrh8z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function below takes some text, runs it through the spaCy tokenizer, then _joins_ the tokens back using a '|' character. The reason why we're doing this is further below."
      ],
      "metadata": {
        "id": "OnRIOcJQr7Z2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "  tokens = spacy_tokenizer(text)\n",
        "  return \"|\".join(tokens)"
      ],
      "metadata": {
        "id": "bYccmYmurpyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess each post in the training corpus. We'll end up with a collection of posts where each token is delimited with '|'."
      ],
      "metadata": {
        "id": "9WdzaBMgtysW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "preprocessed_train_corpus = [preprocess_text(post) for post in train_corpus.data]"
      ],
      "metadata": {
        "id": "4aeEO-k70_Ky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_train_corpus[0]"
      ],
      "metadata": {
        "id": "QvedWJa7twVF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, we'll split the corpus into a training set and validation set.<br>\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html"
      ],
      "metadata": {
        "id": "-EmHEOYKuw_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train_data, val_data, train_labels, val_labels = train_test_split(preprocessed_train_corpus, train_corpus.target, train_size=0.85, random_state=1)"
      ],
      "metadata": {
        "id": "YfXVJY4Nup-3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(train_data), len(val_data))"
      ],
      "metadata": {
        "id": "2YLDTpTHvVYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "At this point, we'll bring in **Keras**. Keras is a deep-learning framework built on top of Tensorflow, and makes it easy to compose models and iterate fast. Most of the time, Keras will provide everything you need but you can drop down to Tensorflow directly for more low-level customization.<br>\n",
        "https://keras.io/<br>\n",
        "https://www.tensorflow.org/\n",
        "<br><br>\n",
        "As an aside, the word _tensor_ in Tensorflow simply refers to a mathematical object. It's a generalization of scalars and vectors.  A scalar is a zero-rank tensor, a vector is a first-rank tensor, and so on."
      ],
      "metadata": {
        "id": "ocV1yHL1wFSn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're going to use Keras' basic tokenizer to split our posts into sequences of tokens with no further processing. By doing this, Keras will generate an internal vocabulary which we can use to encode the posts into vectors as we'll see.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n",
        "<br><br>\n",
        "There are other ways to do this rather than a two-step tokenization process (e.g. use spaCy to encode our posts as integer sequences and pass that directly to Keras) but this is the most straight-forward for our purpose."
      ],
      "metadata": {
        "id": "AfYRfKIfxgd0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, we're initializing a tokenizer to do nothing but split text on the '|' character.\n",
        "<br><br>\n",
        "We're also including an Out-of-Vocabulary token **('OOV')**. Recall that during testing or inference, it's possible for our model to encounter words it didn't see during training. When that happens, the new word is fed into the model as an **'OOV'** token."
      ],
      "metadata": {
        "id": "PJiJ4XTtyymA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "tokenizer = keras.preprocessing.text.Tokenizer(filters=\"\", lower=False, split='|', oov_token='OOV')"
      ],
      "metadata": {
        "id": "CKuD6ZdKyoif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calling _fit_on_texts_ generates an internal vocabulary.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#fit_on_texts"
      ],
      "metadata": {
        "id": "TAYxR3mFz9m2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.fit_on_texts(train_data)"
      ],
      "metadata": {
        "id": "B32YTCEpzse4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can look at the tokenizer's internals using the _get_config_ method.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#get_config\n",
        "<br><br>\n",
        "We can see information such as how many documents were processed to generate the vocabulary, the frequency of each token, and various indices. Btw--_num_words_ does NOT mean the number of words in the vocabulary. It's actually a parameter we can pass to the tokenizer upon initialization to keep the most frequent {_num_words_} words and to dump the rest. Here, we didn't set any limit."
      ],
      "metadata": {
        "id": "BJmlmllG1L53"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.get_config()"
      ],
      "metadata": {
        "id": "x_zOpeir0XuI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vectorization"
      ],
      "metadata": {
        "id": "sDcyK0xC28pj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next step is to vectorize our text with a bag-of-words (BoW) approach.\n",
        "<br><br>\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "**NOTE:**<br>\n",
        "Now that we understand how neural networks work, there are **MUCH** better ways to vectorize text than bag-of-words for neural network models. But since we haven't learned them yet and this demo is just to get a feel of building models, we'll stick with BoW for now.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "zWa3FqPs2P59"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Keras tokenizer's _texts_to_matrix_ method builds a BoW. It can create different BoW types including binary (default), TF-IDF, and others.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer#texts_to_matrix "
      ],
      "metadata": {
        "id": "4gL1CuQT31ku"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the first post using binary. We're using [:1] here because the \n",
        "# tokenizer expects an *array* of sequences.\n",
        "print(train_data[:1])\n",
        "\n",
        "# The resulting binary BoW has a 1 set for every word present in the sequence.\n",
        "binary_bow = tokenizer.texts_to_matrix(train_data[:1])\n",
        "binary_bow"
      ],
      "metadata": {
        "id": "z6nUn8CR2PKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get indices where the binary BoW is set to 1 indicating the associated word is\n",
        "# present in the sequence.\n",
        "import numpy as np\n",
        "present_tokens = np.where(binary_bow[0] == 1)[0]\n",
        "present_tokens"
      ],
      "metadata": {
        "id": "6em9tHd672sY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Retrieve the words.\n",
        "\" \".join(tokenizer.index_word[n] for n in present_tokens)"
      ],
      "metadata": {
        "id": "VFm-EvXV72pu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the first post using TF-IDF and look at the scores for present tokens.\n",
        "# https://numpy.org/doc/stable/user/basics.indexing.html\n",
        "tfidf_bow = tokenizer.texts_to_matrix(train_data[:1], mode='tfidf')\n",
        "print(tfidf_bow)\n",
        "tfidf_bow[0][present_tokens]"
      ],
      "metadata": {
        "id": "W4Hl5tgM72nY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For simplicity, we'll stick to binary BoW. Feel free to experiment with different modes to see if you can squeeze better performance."
      ],
      "metadata": {
        "id": "pYnUCHcw-6IX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In addition to vectorizing the text into binary BoWs, we're going to also store them in Tensorflow **sparse matrices**. Our vocabulary is quite large and for each post, very few indices in each vector will be set to 1. This means we'll have large matrices of mostly zeros which is expensive to store and can be problematic for environments such as this free tier of Colab.\n",
        "<br><br>\n",
        "Tensorflow **sparse tensors** store these types of data structures more efficiently, and Keras can work seamlessly with them.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensor\n",
        "<br><br>\n",
        "In the future, we'll learn different vectorization techniques to create smaller, _dense_ vectors that can pack in more information beyond just simply indicating whether a word is present."
      ],
      "metadata": {
        "id": "Qj_VIlzN_Gn6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize the training dataset.\n",
        "import tensorflow as tf\n",
        "x_train = tf.sparse.from_dense(tokenizer.texts_to_matrix(train_data))"
      ],
      "metadata": {
        "id": "mYlr1yF372j-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The shape of the tensor corresponds to the number of tokenized documents (rows) and vocabulary (columns)."
      ],
      "metadata": {
        "id": "tVGAmShPB9A1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train.shape"
      ],
      "metadata": {
        "id": "Aza4z1N-Ai7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also need to vectorize our labels. Since our goal is multiclass classification, we'll one-hot encode the labels. That is, each label vector will be an array of length 20 (corresponding to the 20 categories) with one index set to 1 to indicate the correct category. The rest will be zero.\n",
        "<br><br>\n",
        "Keras has a _to_categorical_ method to help with this.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/utils/to_categorical"
      ],
      "metadata": {
        "id": "tuU1X6C-AX_q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train = keras.utils.to_categorical(train_labels)"
      ],
      "metadata": {
        "id": "W6903W5TAXXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The shape of the tensor corresponds to the number of documents (rows) and categories (columns)"
      ],
      "metadata": {
        "id": "ztj2c42gC8HZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_train.shape"
      ],
      "metadata": {
        "id": "JFaK9kkO72g2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the first entry in the vectorized labels, we can see its corresponding category.<br>\n",
        "https://numpy.org/doc/stable/reference/generated/numpy.argmax.html"
      ],
      "metadata": {
        "id": "mv4VBdqLDO41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train[0])\n",
        "print(train_corpus.target_names[np.argmax(y_train[0])])"
      ],
      "metadata": {
        "id": "o5ZiYDMPDOK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll vectorize the validation data and labels as well."
      ],
      "metadata": {
        "id": "DEVAZariKGPv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_val = tf.sparse.from_dense(tokenizer.texts_to_matrix(val_data))\n",
        "y_val = keras.utils.to_categorical(val_labels)"
      ],
      "metadata": {
        "id": "FHOdPLOgKE4I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building an Initial Model"
      ],
      "metadata": {
        "id": "Dy95da3vFE6s"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use the **layers** API from Keras to build our models. Through **layers**, we can describe a model layer-by-layer including the number of weights, which activation function to use, add regularization steps, and more.<br>\n",
        "https://keras.io/api/layers/\n",
        "<br><br>\n",
        "The **Sequential** class groups a stack of layers and provides training/inference features:<br>\n",
        "https://keras.io/api/models/sequential/<br>\n",
        "https://keras.io/guides/sequential_model/\n",
        "<br><br>\n",
        "You can alternatively use the **Functional** API for more flexibility but we'll stick with **Sequential** for now.<br>\n",
        "https://keras.io/guides/functional_api/"
      ],
      "metadata": {
        "id": "vlphbd2EFKaR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll build a simple model with two hidden layers. The output layer uses **softmax** since we're performing multiclass classification."
      ],
      "metadata": {
        "id": "eAo_IfEQH_4i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_CATEGORIES = len(train_corpus.target_names)\n",
        "\n",
        "from keras import layers\n",
        "model = keras.Sequential([\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(NUM_CATEGORIES, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "wLCXxxciFJG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After specifying the layers, we'll compile the model and specify which optimizer, loss function, and performance metric we want to use.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#compile"
      ],
      "metadata": {
        "id": "WXnR9adsJMZ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "H5aFL8_XHs4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similar to our previous experience with Scikit-learn, we can train a Keras model by calling its _fit_ method and specifying a number of parameters. Here, we're also passing in our validation data on which the model will evaluate the loss after each epoch.<br>\n",
        "https://www.tensorflow.org/api_docs/python/tf/keras/Sequential#fit"
      ],
      "metadata": {
        "id": "3mxRnPUsKbdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train, epochs=15, batch_size=128, validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "id": "-lP_ATTjHvOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "**Note:**<br>\n",
        "Because we didn't set a fixed seed value and there is some randomness with neural networks, your model training output may look **different** though I don't expect the differences to be dramatic.\n",
        "\n",
        "---\n",
        "<br><br>\n",
        "During training, our model outputted a history of loss and accuracy metrics for both the training set and validation set. We can see the training and validation metrics get better for a certain number of epochs before they start diverging. Performance on the training set keeps improving while performance on the validation set starts degrading at some point, signalling that the model is starting to overfit.\n",
        "<br><br>\n",
        "We can plot this information as well."
      ],
      "metadata": {
        "id": "ugu1HDMGlDtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_losses = history.history['loss']\n",
        "validation_losses = history.history['val_loss']\n",
        "\n",
        "training_accuracy = history.history['accuracy']\n",
        "validation_accuracy = history.history['val_accuracy']\n",
        "\n",
        "epochs = range(1, len(training_losses) + 1)\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "fig, (ax1, ax2) = plt.subplots(2)\n",
        "fig.set_figheight(15)\n",
        "fig.set_figwidth(15)\n",
        "fig.tight_layout(pad=5.0)\n",
        "\n",
        "# Plot training vs. validation loss.\n",
        "ax1.plot(epochs, training_losses, 'bo', label='Training Loss')\n",
        "ax1.plot(epochs, validation_losses, 'b', label='Validation Loss')\n",
        "ax1.title.set_text('Training vs. Validation Loss')\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "\n",
        "# PLot training vs. validation accuracy.\n",
        "ax2.plot(epochs, training_accuracy, 'bo', label='Training Accuracy')\n",
        "ax2.plot(epochs, validation_accuracy, 'b', label='Validation Accuracy')\n",
        "ax2.title.set_text('Training vs. Validation Accuracy')\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "oQ8mHIJutspV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our current model, the way it's trained, has overfit on the data.\n",
        "<br><br>\n",
        "Since we have an idea of when that overfitting begins, we can now train a _new_ model that stops training at or right before that point. In the following cell, we'll retrain an identical model but this time with the number of epochs equalling the point where the divergence began in our previous model.\n",
        "<br><br>\n",
        "Again, your results may look slightly different so modify accordingly."
      ],
      "metadata": {
        "id": "zTGvLLEIyikS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = keras.Sequential([\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(128, activation='relu'),\n",
        "  layers.Dense(NUM_CATEGORIES, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train, y_train, epochs=<DESIRED_EPOCHS_HERE>, batch_size=128, validation_data=(x_val, y_val))"
      ],
      "metadata": {
        "id": "PRiJ_tEbt4Im"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can look at a summary of our model using the _summary_ method.\n"
      ],
      "metadata": {
        "id": "qILFk5Uw0ms_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "g_0RE0_Z5gnB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Looking at the preceding summary, there's an outsized number of parameters in the input layer because the BoW encoding results in a wide vocabulary array. This isn't great.\n",
        "<br><br>\n",
        "If you're wondering where that _param_ number comes from, here's how it's calculated:\n",
        "\n"
      ],
      "metadata": {
        "id": "fhqqSP1aYHjX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Size of vocabulary. The '+ 1' is because the zero index is reserved for padding.\n",
        "v = (len(tokenizer.word_index) + 1)\n",
        "print('Size of BoW array(v): {}'.format(v))\n",
        "\n",
        "n = 128\n",
        "print('Number of units in the input layer(n): {}'.format(n))\n",
        "\n",
        "print('')\n",
        "\n",
        "# The '+ n' accounts for the number of biases. Each unit has one.\n",
        "p = v * n + n\n",
        "print('Number of params in the input layer(p) = v * n + n = {}'.format(p))"
      ],
      "metadata": {
        "id": "7dlCLHXn5j3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can look at the weights in each layer as well using the _get_weights_ method. Here are the weights of the input layer. It's a two-element array where the first contains the non-bias weights and the second the bias weights.<br>\n",
        "https://keras.io/api/layers/base_layer/#getweights-method"
      ],
      "metadata": {
        "id": "YH3l2mH4bEpU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.layers[0].get_weights()"
      ],
      "metadata": {
        "id": "HWhax1i1bDvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And these are the weights from the first layer's first unit without the bias."
      ],
      "metadata": {
        "id": "bjaw1QJJcFL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ws = model.layers[0].get_weights()[0][0]\n",
        "print(len(ws))\n",
        "ws"
      ],
      "metadata": {
        "id": "CK-TxCpK5huo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try our model on the test set."
      ],
      "metadata": {
        "id": "9AXkabPhckAg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_corpus = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'))"
      ],
      "metadata": {
        "id": "0ejr4Haz5hoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "preprocessed_test_corpus = [preprocess_text(post) for post in test_corpus.data]"
      ],
      "metadata": {
        "id": "Gunk7AOYcjgi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test = tf.sparse.from_dense(tokenizer.texts_to_matrix(preprocessed_test_corpus))\n",
        "y_test = keras.utils.to_categorical(test_corpus.target)"
      ],
      "metadata": {
        "id": "Ycyxwq615hhp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since we're evaluating the model on the test set, we'll use the _evaluate_ method.<br>\n",
        "https://keras.io/api/models/model_training_apis/#evaluate-method"
      ],
      "metadata": {
        "id": "utLrxR4fiZXQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results = model.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "CKKmcrVUiYpm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "_evaluate_ returns a two-element list with the loss as the first entry and the metric of interest as the second entry."
      ],
      "metadata": {
        "id": "5RyUN061i7NI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "results"
      ],
      "metadata": {
        "id": "QuRu_OjJ5haJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Random guessing would result in an accuracy of ~5% (since there are 20 categories and the data is balanced), so the results are much better than that. But it's still not very satisfying.\n",
        "<br><br>\n",
        "Let's take a look at a confusion matrix and classification report. To generate those, we'll need the actual predictions from the model which we'll generate using the _predict_ method.\n",
        "<br>\n",
        "https://keras.io/api/models/model_training_apis/#predict-method"
      ],
      "metadata": {
        "id": "8Pmbw6eHjoo6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_probs = model.predict(x_test, verbose=1)"
      ],
      "metadata": {
        "id": "DreHqcfIlJlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output layer ends with a softmax, so each y_pred element is a probability distribution. We need to convert each element into a single category number in order to plot a confusion matrix."
      ],
      "metadata": {
        "id": "EWCMck0smQ8E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Look at the softmax output for the first item.\n",
        "print(y_pred_probs[0])\n",
        "\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# Look at the most probable category for the first item.\n",
        "print('Category with highest probability (for the first item): {}'.format(y_pred[0]))"
      ],
      "metadata": {
        "id": "4JMkq3bKmPO7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "\n",
        "# Not normalizing this time. Just looking at raw numbers.\n",
        "cm = confusion_matrix(test_corpus.target, y_pred)\n",
        "cmd = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=test_corpus.target_names)\n",
        "fig, ax = plt.subplots(figsize=(15, 15))\n",
        "cmd.plot(ax=ax, xticks_rotation='vertical')"
      ],
      "metadata": {
        "id": "6k6PDy44jnnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A few observations:\n",
        "- As before, there's a cluster of brighter squares around the technology-related subjects (pc.hardware, mac.hardware, electronics, etc), and subjects such as athiesm, christianity, guns, and politics being confused for each other which drag the overall accuracy down.\n",
        "- The more focused topics have brighter diagonal squares.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "**NOTE:**<br>\n",
        "Again, your results may differ.\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0dlimV7enpaU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(test_corpus.target, y_pred, target_names=test_corpus.target_names))"
      ],
      "metadata": {
        "id": "BuBgG7RXnoZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at some posts in categories with a high discrepancy between precision and recall."
      ],
      "metadata": {
        "id": "4Ss3NUcco0mx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The category with a high discrepancy.\n",
        "category_of_interest = test_corpus.target_names.index(<DESIRED_TARGET_NAME>)\n",
        "category_of_interest"
      ],
      "metadata": {
        "id": "BDcXZPReEiMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the indices of predictions which matches the category of interest.\n",
        "category_pred = np.where(y_pred == category_of_interest)[0]\n",
        "category_pred"
      ],
      "metadata": {
        "id": "qQxLqBvBl5Fp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the indices of incorrect predictions.\n",
        "incorrect_pred = np.nonzero(test_corpus.target != y_pred)[0]\n",
        "incorrect_pred"
      ],
      "metadata": {
        "id": "F2We4nji3czt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the indices where the model predicted the category of interest, but was wrong.\n",
        "incorrect_category_pred = category_pred[np.in1d(category_pred, incorrect_pred)]\n",
        "incorrect_category_pred"
      ],
      "metadata": {
        "id": "u3mbYVmTpGtv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def incorrect_pred_posts(post_idx):\n",
        "  print(\"Predicted category: {}\".format(test_corpus.target_names[y_pred[post_idx]]))\n",
        "  print(\"Actual category: {}\".format(test_corpus.target_names[test_corpus.target[post_idx]]))\n",
        "  print(\"Post: {}\".format(preprocessed_test_corpus[post_idx]))\n"
      ],
      "metadata": {
        "id": "L-jJZkaup8jo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at a few of the posts.\n",
        "for i in range(10):\n",
        "  incorrect_pred_posts(incorrect_category_pred[i])\n",
        "  print()"
      ],
      "metadata": {
        "id": "afxBzyVpqUIp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Making Another Attempt"
      ],
      "metadata": {
        "id": "UQdbsscu9lbM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before building another model, let's throw away any preprocessed posts with fewer than five words."
      ],
      "metadata": {
        "id": "wRHHhl6wBcge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_short_texts(text, min_len, split_char):\n",
        "  tokens = text.split(split_char)\n",
        "  return len(tokens) >= min_len"
      ],
      "metadata": {
        "id": "clulHnWn_1Ni"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Before filtering short texts (train): {}'.format(len(preprocessed_train_corpus)))\n",
        "\n",
        "# Filter training corpus.\n",
        "z = zip(preprocessed_train_corpus, train_corpus.target)\n",
        "f = filter(lambda t: filter_short_texts(t[0], 5, '|'), z)\n",
        "preprocessed_train_corpus, train_corpus.target = zip(*f)\n",
        "\n",
        "print('After filtering short texts (train): {}'.format(len(preprocessed_train_corpus)))"
      ],
      "metadata": {
        "id": "heugZVcq0lB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Before filtering short texts (test): {}'.format(len(preprocessed_test_corpus)))\n",
        "\n",
        "# Do the same for the test corpus.\n",
        "z = zip(preprocessed_test_corpus, test_corpus.target)\n",
        "f = filter(lambda t: filter_short_texts(t[0], 5, '|'), z)\n",
        "preprocessed_test_corpus, test_corpus.target = zip(*f)\n",
        "\n",
        "print('Before filtering short texts (test): {}'.format(len(preprocessed_test_corpus)))"
      ],
      "metadata": {
        "id": "0nfj-8j6Ey5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Resplit the training data into train/validation sets.\n",
        "train_data, val_data, train_labels, val_labels = train_test_split(preprocessed_train_corpus, train_corpus.target, train_size=0.85, random_state=1)"
      ],
      "metadata": {
        "id": "Q6roZ35M5AFW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-vectorize the training, validation, and test data.\n",
        "x_train = tf.sparse.from_dense(tokenizer.texts_to_matrix(train_data))\n",
        "y_train = keras.utils.to_categorical(train_labels)\n",
        "\n",
        "x_val = tf.sparse.from_dense(tokenizer.texts_to_matrix(val_data))\n",
        "y_val = keras.utils.to_categorical(val_labels)\n",
        "\n",
        "x_test = tf.sparse.from_dense(tokenizer.texts_to_matrix(preprocessed_test_corpus))\n",
        "y_test = keras.utils.to_categorical(test_corpus.target)"
      ],
      "metadata": {
        "id": "dthEyEc_KlxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the next attempt, add another layer with some **dropout** regularization and use **He initialization**.<br>\n",
        "https://keras.io/api/layers/regularization_layers/dropout/<br>\n",
        "https://keras.io/api/layers/initializers/#layer-weight-initializers\n",
        "<br><br>\n",
        "We'll also leverage **early stopping** to halt training once our validation stops improving. This'll save us the trouble of manually training another model with fewer epochs as we did with the previous model. This is done through a **callback**. Here, the **patience** parameter specifies how many epochs to process with no improvement before training stops. Since we saw in the early graphs that validation loss diverges pretty sharply, we're setting it to 1. If you saw that validation tends to plateau for a bit before improving again, you could consider setting a higher value. There are other settings worth reading about as well.<br>\n",
        "https://keras.io/api/callbacks/<br>\n",
        "https://keras.io/api/callbacks/early_stopping/<br>\n",
        "\n"
      ],
      "metadata": {
        "id": "p8noAw_bJ6Ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=1)\n",
        "\n",
        "initializer = tf.keras.initializers.HeNormal()\n",
        "\n",
        "model_next = keras.Sequential([\n",
        "  layers.Dense(128, activation='relu', kernel_initializer=initializer),\n",
        "  layers.Dense(128, activation='relu', kernel_initializer=initializer),\n",
        "  layers.Dense(128, activation='relu', kernel_initializer=initializer),\n",
        "  layers.Dropout(0.3),\n",
        "  layers.Dense(NUM_CATEGORIES, activation='softmax')\n",
        "])\n",
        "\n",
        "model_next.compile(optimizer='rmsprop',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model_next.fit(x_train, y_train, epochs=15, batch_size=128, validation_data=(x_val, y_val), callbacks=[es_callback])"
      ],
      "metadata": {
        "id": "DYg_DefxJ453"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = model_next.evaluate(x_test, y_test)"
      ],
      "metadata": {
        "id": "PfLZhFbbJw1-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_probs = model_next.predict(x_test, verbose=1)\n",
        "y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "# Not normalizing this time. Just looking at raw numbers.\n",
        "cm = confusion_matrix(test_corpus.target, y_pred)\n",
        "cmd = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=test_corpus.target_names)\n",
        "fig, ax = plt.subplots(figsize=(15, 15))\n",
        "cmd.plot(ax=ax, xticks_rotation='vertical')"
      ],
      "metadata": {
        "id": "C9PJYJmZJw52"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(test_corpus.target, y_pred, target_names=test_corpus.target_names))"
      ],
      "metadata": {
        "id": "5NGtBFm2Jw92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_names = test_corpus.target_names.copy()\n",
        "\n",
        "def classify_post(post):\n",
        "  vectorized_post = tokenizer.texts_to_matrix([('|').join(spacy_tokenizer(post))])\n",
        "  probs = model_next.predict(vectorized_post)\n",
        "  pred = np.argmax(probs, axis=1)[0]\n",
        "  return target_names[pred], probs[0][pred]"
      ],
      "metadata": {
        "id": "CFu-FWnwJxB0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Post from r/medicine.\n",
        "s = \"New primary care attending here. Why are all my new patients age 60-80 yo on Ambien? Serious question, why? Was there a strong marketing push at this time frame? Was it given out like candy to anyone who said they had some trouble with sleep? Was there any discussion of risks and duration of therapy? Has anyone had success/tips for weaning them off of it?\"\n",
        "classify_post(s)"
      ],
      "metadata": {
        "id": "od6idkNVJxEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Post from r/space.\n",
        "s = \"James Webb Space Telescope has successfully deployed its forward sunshield pallet! Next up: aft sunshield deployment\"\n",
        "classify_post(s)"
      ],
      "metadata": {
        "id": "34VkfDgMJxII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Post from r/cars.\n",
        "s = \"Cars made in the last 10 years with a 4 Speed Manual Transmission? As per the title really, I’m wondering if any vehicles have been made in the last 10 years that still utilise a 4 speed (or less) manual transmission. My Google research has thus far not turned up any results.\"\n",
        "classify_post(s)"
      ],
      "metadata": {
        "id": "nDs6JhzmVnSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Post from r/electronics.\n",
        "s = \"This project is powered by an ATTiny85. Five of its pins were used, three of them for the MAX7219 module controling the 7-segment display and one for the button and piezo buzzer respectively. The user can give input through the button. A normal short press to count one up and a long 6-second press to reset it to 0. I also added a simple switch. The microcontroller stores the value in its EEPROM so it doesn't lose it when powered off. I used a charger of an old phone as a power supply. My dad was really excited when he got it for Christmas and it should certainly help him quit smoking :)\"\n",
        "classify_post(s)"
      ],
      "metadata": {
        "id": "y3ozjQv-X-G_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So at this point, we have a model that roughly matches the performance of the naive bayes classifier. You could further experiment with a bunch of other things from what we learned:\n",
        "- Use a different tokenizer mode (e.g. count or TF-IDF).\n",
        "- Filter out words based on frequency (e.g. bottom and top 20%).\n",
        "- Train with much more data.\n",
        "- Use another optimizer.\n",
        "- Use more layers (deeper network) or more units in a layer (wider network).\n",
        "- Tweak the regularization.\n",
        "<br><br>\n",
        "\n",
        "That being said, it's going to be difficult to squeeze much more performance because:\n",
        "- Stripped of metadata, a lot of these posts are ambiguous and which a human would have a hard time classifying.\n",
        "- Our BoW encoding is also subpar in that it's large but encodes no information beyond whether a word is present. Especially with the overlapping topics which drag the overall accuracy down, throwing away context makes it much harder to classify.\n",
        "- Because our input vectors are extremely wide and sparse, we're forced to reduce it down aggressively to a manageable number of units in the input layer. If we were to have an input layer with 20,000 units for example (roughly half of the vocabulary size), that layer alone would have over 800 million parameters which is absurd for a problem of this nature.\n",
        "<br><br>\n",
        "But this dataset was used because, beyond putting what we learned into practice, it's important to keep in mind that dirty/low-signal data and information loss during vectorization has a heavy influence on downstream work and performance.\n",
        "<br><br>\n",
        "In the rest of the course, we'll learn vectorization techniques which encode much more information in a much smaller space."
      ],
      "metadata": {
        "id": "dRkyU6yZbZeP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Additional Reading\n",
        "If you're curious about how to build a custom model using the low-level features of Tensorflow, here are a few links to work through:<br>\n",
        "https://www.tensorflow.org/tutorials/customization/basics<br>\n",
        "https://www.tensorflow.org/tutorials/customization/custom_layers<br>\n",
        "https://www.tensorflow.org/tutorials/customization/custom_training_walkthrough<br>\n",
        "https://www.tensorflow.org/guide/keras/customizing_what_happens_in_fit"
      ],
      "metadata": {
        "id": "3zkDEctl53H9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practice"
      ],
      "metadata": {
        "id": "TYC-P_Cjddza"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensorflow comes with a number dataset loaders, one of which is a collection of ~11,000 Reuters news articles in 46 categories.\n",
        "<br><br>\n",
        "Retrieve the data, vectorize both the articles and labels, and build a model to classify the articles."
      ],
      "metadata": {
        "id": "u_2AiecCC1jd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import reuters"
      ],
      "metadata": {
        "id": "TP6aKqZuiAtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Call the load_data method to retrieve the train and test sets. Explore the load_data\n",
        "# method to see what options there are (e.g. limiting the number of words).\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/datasets/reuters/load_data\n",
        "#\n",
        "# NOTE: The load_data method doesn't return arrays of strings, but rather\n",
        "# arrays of integers. Each news article is encoded as a sequence of integers. There's \n",
        "# no need to tokenize. You can recreate the article using get_word_index.\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/datasets/reuters/get_word_index\n",
        "#\n"
      ],
      "metadata": {
        "id": "adbnUba2rvty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize x_train and x_test (i.e. the articles) as some bag of words matrices.\n",
        "# Maybe you can use the Keras Tokenizer?\n",
        "# https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer\n"
      ],
      "metadata": {
        "id": "cPgcP_F--Yn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorize y_train and y_test (i.e. the labels) as one-hot/categorical encodings.\n"
      ],
      "metadata": {
        "id": "VN_4devV_KNK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create your model architecture here.\n",
        "from tensorflow import keras \n",
        "from tensorflow.keras import layers\n",
        "\n",
        "model = keras.Sequential([\n",
        "  # Your layers here\n",
        "])"
      ],
      "metadata": {
        "id": "GIhd7PRf_SHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile your model here specifying an optimizer, loss function, and performance metric.\n"
      ],
      "metadata": {
        "id": "c7sXKzwE_j6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit your model on your test set using early stopping. Optionally divide the test set \n",
        "# into test/validation splits and pass the validation data to the fit method.\n"
      ],
      "metadata": {
        "id": "2jfI0L2N_rzb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you're satisfied, evaluate the model on the test set and see what you get.\n"
      ],
      "metadata": {
        "id": "pzUlD_ev_7Lb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}