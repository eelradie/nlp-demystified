{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlpdemyst-topic-modelling-lda.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nitinpunjabi/nlp-demystified/blob/main/notebooks/nlpdemyst_topic_modelling_lda.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITy3IHHU95uS"
      },
      "source": [
        "# Natural Language Processing Demystified | Latent Dirichlet Allocation\n",
        "https://nlpdemystified.org<br>\n",
        "https://github.com/nitinpunjabi/nlp-demystified"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aes1ZqWZTUa5"
      },
      "source": [
        "# spaCy upgrade and package installation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zSVwiu4YTVDa"
      },
      "source": [
        "At the time this notebook was created, spaCy had newer releases but Colab was still using version 2.x by default. So the first step is to upgrade spaCy.\n",
        "<br><br>\n",
        "**IMPORTANT**<br>\n",
        "If you're running this for free in the cloud rather than using a paid tier or using a local Jupyter server on your machine, then the notebook will *timeout* after a period of inactivity. If that happens and you don't reconnect in time, you will need to upgrade spaCy again and reinstall the requisite statistical package(s).\n",
        "<br><br>\n",
        "Refer to this link on how to run Colab notebooks locally on your machine to avoid this issue:<br>\n",
        "https://research.google.com/colaboratory/local-runtimes.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VstAdWMUWvp"
      },
      "source": [
        "!pip install -U spacy==3.*\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogOLLHK-Errs"
      },
      "source": [
        "!python -m spacy info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DKZgKn9TTc9Z"
      },
      "source": [
        "For topic modelling, we'll use **Gensim**, a popular topic modelling library originally authored by Radim Řehůřek. It has implementations of LDA and other models we'll use later in the course.<br>\n",
        "https://radimrehurek.com/gensim/index.html"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "spacy.prefer_gpu()"
      ],
      "metadata": {
        "id": "p-VUB3FQ_Juz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1t-eUJvZCGiw"
      },
      "source": [
        "# Upgrade gensim in case.\n",
        "!pip install -U gensim==4.*"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUqudgVeCfbM"
      },
      "source": [
        "# First pass at building an LDA topic model for our corpus"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mHBDR4ZqVvwY"
      },
      "source": [
        "We'll use a corpus of over 2,000 Associated Press news articles compiled by David M. Blei, one of the authors of the original LDA paper<br>\n",
        "<br>\n",
        "The original paper:<br>\n",
        "https://dl.acm.org/doi/10.5555/944919.944937\n",
        "<br>\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7ji4bewXBxS"
      },
      "source": [
        "# https://docs.python-requests.org/en/master/\n",
        "import requests"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQGh-5xrXUWM"
      },
      "source": [
        "# Retrieve the articles and put them in a list.\n",
        "url = 'https://raw.githubusercontent.com/nitinpunjabi/nlp-demystified/main/datasets/ap_articles.txt'\n",
        "response = requests.get(url)\n",
        "articles = response.text.splitlines()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JS95EiKNXUOF"
      },
      "source": [
        "articles[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVv67oSOXUG0"
      },
      "source": [
        "# Like before, if we want to use spaCy's tokenizer, we need \n",
        "# to create a callback. In this case, we'll start off with a\n",
        "# blank tokenizer (i.e. no parsing, tagging, etc).\n",
        "nlp = spacy.blank('en')\n",
        "\n",
        "# For this exercise, we'll remove punctuation and spaces (which\n",
        "# includes newlines), filter for tokens consisting of alphabetic\n",
        "# characters, and return the token text.\n",
        "def spacy_tokenizer(doc):\n",
        "  return [t.text for t in nlp(doc) if \\\n",
        "          not t.is_punct and \\\n",
        "          not t.is_space and \\\n",
        "          t.is_alpha]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mknW0nlXT_b"
      },
      "source": [
        "# Tokenize all the articles\n",
        "%%time\n",
        "tokenized_articles = []\n",
        "for a in articles:\n",
        "  tokenized_articles.append(spacy_tokenizer(a))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYNK7Nd-cLsZ"
      },
      "source": [
        "print(tokenized_articles[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkopX2P4UqDK"
      },
      "source": [
        "To start off, we'll go with 20 topics. With most topic models including LDA, there isn't a clear recipe on how to pick the optimal number of topics. The nature and composition of the data (e.g. average length of each document) has a major impact on how many topics are *interpretable* by a human. Often, it's best to go with something reasonable to begin with and then try different topic numbers. With ~2,500 documents, 20-50 topics can usually give someone a good idea of the content."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9RbTz3OXTuM"
      },
      "source": [
        "from gensim import models, corpora\n",
        "NUM_TOPICS = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgCbr9SJZxDQ"
      },
      "source": [
        "After tokenizing our text, we use Gensim to construct a **Dictionary** mapping words to their integer IDs.<br>\n",
        "https://radimrehurek.com/gensim/corpora/dictionary.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EP2db-H8cUwb"
      },
      "source": [
        "# Build a Dictionary of word<-->id mappings.\n",
        "dictionary = corpora.Dictionary(tokenized_articles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XyAHgUxEaXVf"
      },
      "source": [
        "The next step is to create a frequency bag-of-words from each article.\n",
        "https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.doc2bow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZYpRy9W6cWAK"
      },
      "source": [
        "corpus_bow = [dictionary.doc2bow(article) for article in tokenized_articles]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KD9khr0RbBTq"
      },
      "source": [
        "Finally, we'll generate our base LDA model. Gensim's LDA model has a large number of optional parameters but for now, we'll keep it simple.<br>\n",
        "https://radimrehurek.com/gensim/models/ldamodel.html?highlight=lda#module-gensim.models.ldamodel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AP0MS3n7dxE_"
      },
      "source": [
        "%%time\n",
        "lda_model = models.LdaModel(corpus=corpus_bow, num_topics=NUM_TOPICS, id2word=dictionary, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ecFL_MSb9wp"
      },
      "source": [
        "Once our model is generated, we can view the topics inferred. By default, the model's *print_topics* method shows the top 20 topics and each topic's ten most significant words.<br>\n",
        "https://radimrehurek.com/gensim/models/ldamodel.html?highlight=lda#gensim.models.ldamodel.LdaModel.print_topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lFTFPOb4eKUi"
      },
      "source": [
        "lda_model.print_topics()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYmfb5YGcSP8"
      },
      "source": [
        "The first pass is pretty awful. The topics are dominated by stop words such that they essentially look all the same. Let's see if we can do better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kf0X-w47svTF"
      },
      "source": [
        "# Improving preprocessing for better results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AkI5wxWccz8U"
      },
      "source": [
        "For our next attempt, we'll\n",
        "- remove stop words using the default list. Given this is a corpus of news articles, there may be other stop words to consider such as salutations (\"Mr\", \"Mrs\"), and words related to quotes and thoughts (\"say\", \"think\"). But for this, we'll stick to defaults unless we see reason to do otherwise.\n",
        "- consider only the words the spaCy tagger flags as *nouns, verbs,* and *adjectives*. Including words with only certain POS tags is a common approach to improving topic models.\n",
        "- take the lemma."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjixsDXigIJ_"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def spacy_tokenizer_w_pos(doc):\n",
        "  return [t.lemma_ for t in nlp(doc) if \\\n",
        "          t.is_alpha and \\\n",
        "          not t.is_punct and \\\n",
        "          not t.is_space and \\\n",
        "          not t.is_stop and \\\n",
        "          t.pos_ in ['NOUN', 'VERB', 'ADJ']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLqKeoy9FQED"
      },
      "source": [
        "# We'll need to retokenize everything and rebuild the BOWs. Because we're now\n",
        "# using the POS tagger, this will take longer. The \"w_pos\" in the variable \n",
        "# names below just means \"with part-of-speech\".\n",
        "%%time\n",
        "tokenized_articles_w_pos = [spacy_tokenizer_w_pos(a) for a in articles]\n",
        "dictionary_w_pos = corpora.Dictionary(tokenized_articles_w_pos)\n",
        "corpus_bow_w_pos = [dictionary_w_pos.doc2bow(article) for article in tokenized_articles_w_pos]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5sNd_PZypu13"
      },
      "source": [
        "lda_model = models.LdaModel(corpus=corpus_bow_w_pos, num_topics=NUM_TOPICS, id2word=dictionary_w_pos, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aG5iFkrQqyx5"
      },
      "source": [
        "lda_model.print_topics()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ckwYRIqgOtB"
      },
      "source": [
        "This is better but there are still a few low-signal words dominating topics such as \"said\" lemmatized to \"say\" which makes sense for a news corpus. Perhaps trimming the vocabulary and tuning the model parameters themselves can lead to something more interpretable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_8oBuWxvqdl"
      },
      "source": [
        "# Trimming low- and high-frequency words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFDI1BSLgxJw"
      },
      "source": [
        "One thing we can try is filtering out rare and common tokens.\n",
        "https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary.filter_extremes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YQctCWVhnL6"
      },
      "source": [
        "# The size of the dictionary before filtering.\n",
        "len(dictionary_w_pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8tzEnZKyfeC"
      },
      "source": [
        "The filtering is a bit idiosyncratic. The lower bound is an *absolute* number, and the upper bound is a *percentage*. Here, we're saying filter out words which occur in fewer than five documents and more than 50% of the documents."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyCG8tLIp2QC"
      },
      "source": [
        "dictionary_w_pos.filter_extremes(no_below=5, no_above=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AvypKffhpyR"
      },
      "source": [
        "# The size of the dictionary after filtering.\n",
        "len(dictionary_w_pos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWomtBFzhuO5"
      },
      "source": [
        "# Rebuild bag of words.\n",
        "corpus_bow_w_pos_filtered = [dictionary_w_pos.doc2bow(article) for article in tokenized_articles_w_pos]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVtALC9yYB9Z"
      },
      "source": [
        "This time, we're passing additional arguments when building the model. *alpha* is the prior on each topic's probability, *eta* is the prior on each word's probability, and *passes* is the number of complete passes through the corpus during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8_-mIdSvqUc"
      },
      "source": [
        "%%time\n",
        "lda_model = models.ldamodel.LdaModel(corpus=corpus_bow_w_pos_filtered,\n",
        "                                     id2word=dictionary_w_pos,\n",
        "                                     num_topics=NUM_TOPICS,\n",
        "                                     passes=10,\n",
        "                                     alpha='auto',\n",
        "                                     eta='auto',\n",
        "                                     random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iR2xCvNZvqDn"
      },
      "source": [
        "lda_model.print_topics()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tR9ELznir0va"
      },
      "source": [
        "articles[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "auRQbV8Ajaz8"
      },
      "source": [
        "We can look at the topic distribution comprising a given article.<br>\n",
        "https://radimrehurek.com/gensim/models/ldamodel.html#gensim.models.ldamodel.LdaModel.get_document_topics"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OiuRTLEPtOod"
      },
      "source": [
        "sorted(lda_model.get_document_topics(corpus_bow_w_pos_filtered[0]), key=lambda tup: tup[1])[::-1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-063hSnjvp7F"
      },
      "source": [
        "lda_model.show_topic(16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCr_9vWPvuU9"
      },
      "source": [
        "The results of this model look the best so far and we can see a human-interpretable link between the distribution of topics in a document, the distribution of words in each topic, and the content of the document itself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRCf02nVvpfW"
      },
      "source": [
        "# Evaluation and Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TXlK5gebUjB"
      },
      "source": [
        "## Measuring topic models with coherence.\n",
        "\n",
        "If a topic is a mixture of particular words, then one way to measure how semantically coherent a topic is to calculate co-occurrence among the words. That is, how often the top words in a topic co-occur together among the documents versus how often they occur independently.\n",
        "\n",
        "Gensim's **Coherence Model** offers coherence implemented as a pipeline:<br>\n",
        "https://radimrehurek.com/gensim/models/coherencemodel.html\n",
        "<br>\n",
        "<br>\n",
        "See this paper for a detailed description of the pipeline as well as different co-occurence measures proposed (here, we are using the default *c_v* measure):<br>\n",
        "http://svn.aksw.org/papers/2015/WSDM_Topic_Evaluation/public.pdf\n",
        "<br>\n",
        "<br>\n",
        "Topic model evaluation is a difficult subject with no clear quantitative approach. A higher c_v measure doesn't necessarily translate to a higher *qualitative* model. That is, the score a human would give looking at the topic words and how interpretable they are. It's very possible to favour a *lower* scoring model because it serves a particular purpose better. Just keep that in mind. See this video for the problems with quantitative topic model evaluation:<br>\n",
        "[Matti Lyra - Evaluating Topic Models](https://www.youtube.com/watch?v=UkmIljRIG_M)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TY6ulpzgatBl"
      },
      "source": [
        "from gensim.models.coherencemodel import CoherenceModel"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHBp-ZazNZRJ"
      },
      "source": [
        "# Let's check out the coherence of our current model with 20 topics.\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=tokenized_articles_w_pos, dictionary=dictionary_w_pos, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aejymkMOvpXm"
      },
      "source": [
        "# Train another model with 30 topics to see whether the coherence score improves.\n",
        "%%time\n",
        "lda_model_30 = models.ldamodel.LdaModel(corpus=corpus_bow_w_pos_filtered,\n",
        "                                        id2word=dictionary_w_pos,\n",
        "                                        num_topics=30,\n",
        "                                        passes=10,\n",
        "                                        alpha='auto',\n",
        "                                        eta='auto',\n",
        "                                        random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4cg_Fd4yzKC"
      },
      "source": [
        "coherence_lda_30 = CoherenceModel(model=lda_model_30, texts=tokenized_articles_w_pos, dictionary=dictionary_w_pos, coherence='c_v')\n",
        "print('\\nCoherence Score: ', coherence_lda_30.get_coherence())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDtZWbh2d5Pv"
      },
      "source": [
        "We improved the coherence score by a couple of percentage points by increasing the number of topics. One common technique is to try a bunch of different *num_topics* values, plot the coherence score for each, then choose the num_topics with the highest score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kfunq1Su8d1r"
      },
      "source": [
        "## Human evaluation\n",
        "Because the quantitative metrics aren't entirely correlated with quality, human judgment still plays a large role in topic model evaluation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crPK6zKfC1gS"
      },
      "source": [
        "We can look at the topic words to see how interpretable they are..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxPbME-L8d1s"
      },
      "source": [
        "lda_model.show_topic(19)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qYfEicOH8d1t"
      },
      "source": [
        "...or visualize them with word clouds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIVix0mQ8d1t"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qY6uzIW8d1t"
      },
      "source": [
        "def render_word_cloud(model, rows, cols, max_words):\n",
        "  word_cloud = WordCloud(background_color='white', max_words=max_words, prefer_horizontal=1.0)\n",
        "  fig, axes = plt.subplots(rows, cols, figsize=(15,15))\n",
        "\n",
        "  for i, ax in enumerate(axes.flatten()):\n",
        "      fig.add_subplot(ax)\n",
        "      topic_words = dict(model.show_topic(i))\n",
        "      word_cloud.generate_from_frequencies(topic_words)\n",
        "      plt.gca().imshow(word_cloud, interpolation='bilinear')\n",
        "      plt.gca().set_title('Topic {id}'.format(id=i))\n",
        "      plt.gca().axis('off')\n",
        "\n",
        "  plt.axis('off')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3e6HjGtzNnG"
      },
      "source": [
        "# Here we'll visualize the first nine topics.\n",
        "render_word_cloud(lda_model, 3, 3, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5lzddAbCv3vt"
      },
      "source": [
        "There are also subjective tests like *word intrusion* and *topic intrusion*. Word intrusion is taking words which belong to a topic, injecting a word from another topic into the collection, and seeing whether a human can easily identify the intruder word. The more easily the intruder word is spotted, the more well-formed the topic. For example, which word doesn't belong in this topic?<br>\n",
        "*{apple, lemon, tomato, horse, grape}*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FpBxrPcGEOcN"
      },
      "source": [
        "# Finding similar documents."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sJtKEzTE8TSE"
      },
      "source": [
        "Gensim has a **similarities** module which can build an index for a given set of documents. Here, we're using **MatrixSimilarity** which computes cosine similarity across a corpus and stores them in an index.<br>\n",
        "https://radimrehurek.com/gensim/similarities/docsim.html#gensim.similarities.docsim.MatrixSimilarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq9EYQJWkib2"
      },
      "source": [
        "from gensim import similarities\n",
        "lda_index = similarities.MatrixSimilarity(lda_model[corpus_bow_w_pos_filtered], num_features=len(dictionary_w_pos))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHorc8fN9VHu"
      },
      "source": [
        "Here's a utility function to help retrieve the *first_m_words* of the *top_n* most similar documents. If you're curious about the \\_\\_getitem\\__ method on the LDA Model class, you can find the code here:<br>\n",
        "https://github.com/RaRe-Technologies/gensim/blob/master/gensim/models/ldamodel.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6hIGoVYF6Rb"
      },
      "source": [
        "def get_similar_documents(index, bow, model, article_id, top_n=5, first_m_words=300):\n",
        "  # bow[article_id] gets the specific bag-of-words for the given article.\n",
        "  # model[bow[article_id]] retrieves the topic distribution for the BOW.\n",
        "  # index[model[bow[article_id]]] compares the topic distribution for the BOW against the similarity index previously computed.\n",
        "  similar_docs = index[model[bow[article_id]]]\n",
        "  top_n_docs = sorted(enumerate(similar_docs), key=lambda item: -item[1])[1:top_n+1]\n",
        "  \n",
        "  # Return a list of tuples with each tuple: (article id, similarity score, first_m_words of article)\n",
        "  return list(map(lambda entry: (entry[0], entry[1], articles[entry[0]][:first_m_words]), top_n_docs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yn3pWEJZH2GG"
      },
      "source": [
        "article_id = 0\n",
        "articles[article_id]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c4GV6jxI-Q8i"
      },
      "source": [
        "get_similar_documents(lda_index, corpus_bow_w_pos_filtered, lda_model, article_id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8eWhXxhBEl7"
      },
      "source": [
        "We can also query for documents similar to unseen documents. Below are short, actual blurbs from 2021 involving stock options and crime. Because of the subject matter, it's relatively easy to find similar articles to these even in a corpus from the 1980s like this Associated Press collection. But keep in mind that if you query with short articles about subjects like cryptocurrencies and social media, you probably won't find good matches. This is another aspect to keep in mind when thinking about your data and use cases."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rs4DF3CqODIp"
      },
      "source": [
        "#d = \"Capricorn Business Acquisitions Inc. (TSXV: CAK.H) (the “Company“) is pleased to announce that its board has approved the issuance of 70,000 stock options (“Stock Options“) to directors on April 19, 2020.\"\n",
        "d = \"DEA agent sentenced to 12 years in prison for conspiring with Colombian drug cartel.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Elkb2lbnOJZ_"
      },
      "source": [
        "doc_vec = dictionary_w_pos.doc2bow(spacy_tokenizer_w_pos(d))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Lo5rwUjOxnX"
      },
      "source": [
        "similar_docs = lda_index[lda_model[doc_vec]]\n",
        "top_n_docs = sorted(enumerate(similar_docs), key=lambda item: -item[1])[1:5+1]\n",
        "top_n_docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZTCZEv2PJpY"
      },
      "source": [
        "articles[top_n_docs[0][0]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSg41vJ87ExQ"
      },
      "source": [
        "# Look at the topics comprising the query document.\n",
        "topics = sorted(lda_model[doc_vec], key=lambda tup: -tup[1])[:10]\n",
        "topics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91Zs57c175Up"
      },
      "source": [
        "lda_model.show_topic(topics[0][0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NMh0xLVmuKwW"
      },
      "source": [
        "# Closing Thoughts and things to explore.\n",
        "- Gensim infers topic and word distributions through [Variational Bayes (VB)](https://en.wikipedia.org/wiki/Variational_Bayesian_methods), not Gibbs Sampling. From the topics I've seen, Gibbs Sampling tends to lead to more interpretable topics, but VB is faster and Gensim offers the additional benefits of streaming documents, online learning, and training across a cluster of machines.\n",
        "- Another topic modelling library, [Mallet](http://mallet.cs.umass.edu/), infers through Gibbs Sampling but is Java-based. Unfortunately, Gensim 4.0+ no longer offers a wrapper around Mallet. But if you're comfortable with Java, it may be worth exploring.\n",
        "- Scikit-learn offers an [LDA model](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html). Maybe as an exercise, try using this LDA model on the [20 Newsgroups](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) dataset.\n",
        "- [pyLDAvis](https://github.com/bmabey/pyLDAvis) is another means of visualizing topic models. You can see it in action in this [notebook](https://nbviewer.jupyter.org/github/bmabey/pyLDAvis/blob/master/notebooks/pyLDAvis_overview.ipynb). See if you can get it working on your own topic model.\n",
        "- LDA tends to work better on longer documents, and whether a topic model is \"good\" depends on your use case rather than strictly on a quantitative metric."
      ]
    }
  ]
}